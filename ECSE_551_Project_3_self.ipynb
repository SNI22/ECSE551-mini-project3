{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jA04fuOR3Dy"
   },
   "source": [
    "# ECSE 551 MP3 G17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PtEpsotR6FH"
   },
   "source": "## 定义常量、导入外部库和数据集、准备CUDA设备"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn24H1_YQsbg",
    "outputId": "532b5f40-fc9d-42b0-bc20-21a78020b268",
    "ExecuteTime": {
     "end_time": "2024-12-05T01:29:10.368318Z",
     "start_time": "2024-12-05T01:29:06.030522Z"
    }
   },
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.benchmark.examples.spectral_ops_fuzz_test import DEVICE_NAMES\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 设置设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# 设置训练的batch大小\n",
    "THE_BATCH_SIZE = 32\n",
    "\n",
    "# 检测操作系统并设置基础路径\n",
    "if platform.system() == 'Windows':\n",
    "    base_path = 'F:\\\\Personal_file\\\\work_area\\\\Study\\\\ECSE_551\\\\Project3\\\\DataSet'\n",
    "    # 定义全局常量和文件路径\n",
    "    TRAIN_PKL_FILE = os.path.join(base_path + '\\\\Train.pkl')\n",
    "    TRAIN_LABEL_FILE = os.path.join(base_path + '\\\\Train_labels.csv')\n",
    "    TEST_PKL_FILE = os.path.join(base_path + '\\\\Test.pkl')\n",
    "    SUBMISSION_FILE = os.path.join(base_path + '\\\\Submission' + '-CNN-2' + '.csv')\n",
    "else:\n",
    "    base_path = os.path.join('/mnt/f/Personal_file/work_area/Study/ECSE_551/Project3/DataSet')\n",
    "    TRAIN_PKL_FILE = os.path.join(base_path + '/Train.pkl')\n",
    "    TRAIN_LABEL_FILE = os.path.join(base_path + '/Train_labels.csv')\n",
    "    TEST_PKL_FILE = os.path.join(base_path + '/Test.pkl')\n",
    "    SUBMISSION_FILE = os.path.join(base_path + '/Submission' + '-CNN-2' + '.csv')\n",
    "\n",
    "print(\"Train PKL File Path:\", TRAIN_PKL_FILE)\n",
    "print(\"Train Label File Path:\", TRAIN_LABEL_FILE)\n",
    "print(\"Test PKL File Path:\", TEST_PKL_FILE)\n",
    "print(\"Submission File Path:\", SUBMISSION_FILE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train PKL File Path: F:\\Personal_file\\work_area\\Study\\ECSE_551\\Project3\\DataSet\\Train.pkl\n",
      "Train Label File Path: F:\\Personal_file\\work_area\\Study\\ECSE_551\\Project3\\DataSet\\Train_labels.csv\n",
      "Test PKL File Path: F:\\Personal_file\\work_area\\Study\\ECSE_551\\Project3\\DataSet\\Test.pkl\n",
      "Submission File Path: F:\\Personal_file\\work_area\\Study\\ECSE_551\\Project3\\DataSet\\Submission-CNN-2.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mdk157xdTF-z"
   },
   "source": "## 正式开始"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYNPfIdAUr7V"
   },
   "source": "### 尝试读取训练数据，并显示第一张图片"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "638MRjaPTEPb",
    "outputId": "35284fd4-1419-4080-d192-794c61abef6f",
    "ExecuteTime": {
     "end_time": "2024-12-05T01:29:16.420788Z",
     "start_time": "2024-12-05T01:29:16.236754Z"
    }
   },
   "source": [
    "# 加载训练数据和标签\n",
    "with open(TRAIN_PKL_FILE, 'rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "\n",
    "labels_df = pd.read_csv(TRAIN_LABEL_FILE)\n",
    "labels_dict = dict(zip(labels_df['id'], labels_df['class']))\n",
    "train_labels = [labels_dict.get(i + 1, 'Unknown') for i in range(len(train_data))]\n",
    "\n",
    "# 检查数据结构\n",
    "print(type(train_data))  # 查看数据类型\n",
    "print(\"Data dimensions:\", train_data[0].shape)  # 检查数据维度\n",
    "\n",
    "# 显示第一张图片（假设是灰度图像）\n",
    "plt.imshow(train_data[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {train_labels[0]}\")\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Data dimensions: (1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAk4ElEQVR4nO3de3BU9f3/8dcmwgYw2RhCbhIQEETkYouQpgLFEgmxdQwig4W24FisGFoVLzQWAWudqLVIxRRsrUan4gXLpVLFKpjgJYCgiFSNhIYShATFIQsJSTA5vz/4uV9XEuBz2M0nCc/HzJkhu+e1553DysuT3XzW4ziOIwAAWliE7QEAAGcmCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCgg4Tbt27ZLH49FDDz0UsscsLCyUx+NRYWFhyB4TaG0oIJyRCgoK5PF4tHnzZtujhMX8+fPl8XiO26KiomyPBgScZXsAAOGzePFinX322YGvIyMjLU4DBKOAgHbsmmuuUXx8vO0xgCbxIzigGfX19Zo7d66GDh0qn8+nLl26aOTIkXrjjTeazTz88MPq2bOnOnXqpB/84Afavn37cft88sknuuaaaxQXF6eoqChdcskl+uc//3nSeWpqavTJJ5/oiy++OOXvwXEc+f1+seg9WiMKCGiG3+/X448/rtGjR+uBBx7Q/Pnz9fnnnyszM1Nbt249bv+nn35ajzzyiHJycpSbm6vt27frhz/8oSorKwP7/Oc//9H3vvc9ffzxx/rNb36jP/7xj+rSpYuys7O1YsWKE86zadMmXXjhhXr00UdP+Xvo3bu3fD6foqOj9dOf/jRoFsA2fgQHNOOcc87Rrl271LFjx8Bt06dPV//+/bVo0SL97W9/C9q/tLRUO3bs0LnnnitJGjdunNLS0vTAAw9owYIFkqSbb75ZPXr00Lvvviuv1ytJuummmzRixAjNnj1b48ePD9nsM2fOVHp6urxer958803l5+dr06ZN2rx5s2JiYkJyHOB0UEBAMyIjIwMv2jc2NurgwYNqbGzUJZdcovfee++4/bOzswPlI0nDhw9XWlqaXn75ZS1YsEBffvml1q1bp9/97nc6dOiQDh06FNg3MzNT8+bN02effRb0GN80evToU/5R2s033xz09YQJEzR8+HBNmTJFf/7zn/Wb3/zmlB4HCCd+BAecwFNPPaXBgwcrKipKXbt2Vbdu3fSvf/1LVVVVx+3bt2/f427r16+fdu3aJenYFZLjOLr77rvVrVu3oG3evHmSpP3794fte5k8ebKSkpL0+uuvh+0YgAmugIBm/P3vf9e0adOUnZ2tO+64QwkJCYqMjFReXp527txp/HiNjY2SpNtvv12ZmZlN7nP++eef1swnk5qaqi+//DKsxwBOFQUENOPFF19U7969tXz5cnk8nsDtX1+tfNuOHTuOu+3TTz/VeeedJ+nYGwIkqUOHDsrIyAj9wCfhOI527dql73znOy1+bKAp/AgOaMbXr/9883WXjRs3qri4uMn9V65cqc8++yzw9aZNm7Rx40ZlZWVJkhISEjR69Gg99thj2rdv33H5zz///ITzmLwNu6nHWrx4sT7//HONGzfupHmgJXAFhDPaE088oTVr1hx3+80336wf//jHWr58ucaPH68f/ehHKisr05IlSzRgwAAdPnz4uMz555+vESNGaMaMGaqrq9PChQvVtWtX3XnnnYF98vPzNWLECA0aNEjTp09X7969VVlZqeLiYu3Zs0cffPBBs7Nu2rRJl112mebNm6f58+ef8Pvq2bOnJk2apEGDBikqKkpvvfWWnnvuOV188cX65S9/eeonCAgjCghntMWLFzd5+7Rp0zRt2jRVVFToscce06uvvqoBAwbo73//u5YtW9bkIqE///nPFRERoYULF2r//v0aPny4Hn30USUnJwf2GTBggDZv3qx77rlHBQUFOnDggBISEvSd73xHc+fODdn3NWXKFL3zzjv6xz/+odraWvXs2VN33nmnfvvb36pz584hOw5wOjwOvyINALCA14AAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCi1f0eUGNjo/bu3avo6Oig5U8AAG2D4zg6dOiQUlJSFBHR/HVOqyugvXv3KjU11fYYAIDTVF5eru7duzd7f6sroOjoaNsjAHDBzWcMxcXFGWcWLlxonNm7d69xBqfvZP+eh62A8vPz9Yc//EEVFRUaMmSIFi1apOHDh580x4/dgLYpKirKONOpUyfjzIl+pIPW5WT/noflb/L555/XrFmzNG/ePL333nsaMmSIMjMzw/phWwCAtiUsBbRgwQJNnz5d1113nQYMGKAlS5aoc+fOeuKJJ8JxOABAGxTyAqqvr9eWLVuCPnArIiJCGRkZTX6OSl1dnfx+f9AGAGj/Ql5AX3zxhRoaGpSYmBh0e2JioioqKo7bPy8vTz6fL7DxDjgAODNYfzUvNzdXVVVVga28vNz2SACAFhDyd8HFx8crMjJSlZWVQbdXVlYqKSnpuP29Xq+8Xm+oxwAAtHIhvwLq2LGjhg4dqrVr1wZua2xs1Nq1a5Wenh7qwwEA2qiw/B7QrFmzNHXqVF1yySUaPny4Fi5cqOrqal133XXhOBwAoA0KSwFNmjRJn3/+uebOnauKigpdfPHFWrNmzXFvTAAAnLk8juM4tof4Jr/fL5/PZ3sMoF2IiYlxlbvrrruMM0VFRcaZm266yTizdOlS48zLL79snJGkqqoqVzkcU1VVdcLnoPV3wQEAzkwUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsCIsq2EDCL3OnTsbZy6//HJXx4qPjzfO3HvvvcaZCy+80Dhz0UUXGWfcfD+S9Nhjjxln6uvrXR3rTMQVEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKxgNWygjYiIMP//xVGjRrk61oABA4wz27ZtM86ce+65xpnVq1cbZ9atW2eckaSGhgZXOZwaroAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoWIwXaiJqaGuPM888/7+pY+/fvN848++yzxhk3i54OHz7cOLN9+3bjjCR99NFHxpnIyEjjTFxcnHHm8OHDxhlJOnLkiKtcOHAFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWsBgp0EY0NjYaZ4qLi10d68MPPzTOzJgxwzgzf/5848yvf/1r48yhQ4eMM5LkOI5x5o477jDOjBw50jjj5u9Ikh5//HHjTGlpqatjnQxXQAAAKyggAIAVIS+g+fPny+PxBG39+/cP9WEAAG1cWF4Duuiii/T666//30HO4qUmAECwsDTDWWedpaSkpHA8NACgnQjLa0A7duxQSkqKevfurSlTpmj37t3N7ltXVye/3x+0AQDav5AXUFpamgoKCrRmzRotXrxYZWVlGjlyZLNvg8zLy5PP5wtsqampoR4JANAKhbyAsrKyNHHiRA0ePFiZmZl6+eWXdfDgQb3wwgtN7p+bm6uqqqrAVl5eHuqRAACtUNjfHRAbG6t+/fo1+4tMXq9XXq833GMAAFqZsP8e0OHDh7Vz504lJyeH+1AAgDYk5AV0++23q6ioSLt27dI777yj8ePHKzIyUj/5yU9CfSgAQBsW8h/B7dmzRz/5yU904MABdevWTSNGjNCGDRvUrVu3UB8KANCGhbyAnnvuuVA/JACXOnXq5Cp33XXXGWc2bdpknJk0aZJxZuDAgcaZiy++2DgjqcVen/7yyy+NMykpKa6O1bVrV+MMi5ECANoVCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFgR9g+kA2CP28/hio2NNc5s377dONOhQwfjzBVXXGGcufHGG40zkrR7927jTFlZmXFm8uTJxpmICHfXDzU1Na5y4cAVEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKxgNWygjYiMjDTOjBkzxtWxzjnnHFc5U3fccYdxpra21jjz9NNPG2ck6amnnjLO/Pe//zXOHD582DjTHnAFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWsBgp0EakpqYaZyZOnOjqWB999JFx5vrrrzfOdO/e3TgzZ84c48wrr7xinHHLzcKiHo/HOOM4jnGmteEKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCs8DitbEU7v98vn89newygXfjZz37mKrdo0SLjjJt/SkpLS40zbhb7/PnPf26ckaTy8nJXOVO//OUvjTPvvvuuq2N98MEHxpmGhgZXx6qqqlJMTEyz93MFBACwggICAFhhXEDr16/XlVdeqZSUFHk8Hq1cuTLofsdxNHfuXCUnJ6tTp07KyMjQjh07QjUvAKCdMC6g6upqDRkyRPn5+U3e/+CDD+qRRx7RkiVLtHHjRnXp0kWZmZmqra097WEBAO2H8SeiZmVlKSsrq8n7HMfRwoULNWfOHF111VWSpKefflqJiYlauXKlrr322tObFgDQboT0NaCysjJVVFQoIyMjcJvP51NaWpqKi4ubzNTV1cnv9wdtAID2L6QFVFFRIUlKTEwMuj0xMTFw37fl5eXJ5/MFNjefew8AaHusvwsuNzdXVVVVga2l3ncPALArpAWUlJQkSaqsrAy6vbKyMnDft3m9XsXExARtAID2L6QF1KtXLyUlJWnt2rWB2/x+vzZu3Kj09PRQHgoA0MYZvwvu8OHDQctnlJWVaevWrYqLi1OPHj10yy236Pe//7369u2rXr166e6771ZKSoqys7NDOTcAoI0zLqDNmzfrsssuC3w9a9YsSdLUqVNVUFCgO++8U9XV1brhhht08OBBjRgxQmvWrFFUVFTopgYAtHksRgq0Y1//Pp6pX/ziF8YZN28g+uaP60/VbbfdZpxZtWqVcUaS3n77beNMXFycceaZZ54xzhQUFBhnJGn27NnGmZqaGlfHYjFSAECrRAEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBXGH8cAoO3497//7Sr35ptvGmeOHDlinGloaDDOVFRUGGeuuOIK44wkTZw40TjTr18/40x9fb1x5p133jHOSO7+nsKFKyAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsILFSIF2zO3Ck61pwcpvc7MI5wcffODqWGPGjDHOjBgxwjjTtWtX40x7wBUQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFjhcRzHsT3EN/n9fvl8PttjAECLiY2NNc7cd999ro51ySWXGGfS0tJcHauqqkoxMTHN3s8VEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYcZbtAdA8j8djnGlla8sCOAUJCQnGmaFDh7o6Vnl5uatcOHAFBACwggICAFhhXEDr16/XlVdeqZSUFHk8Hq1cuTLo/mnTpsnj8QRt48aNC9W8AIB2wriAqqurNWTIEOXn5ze7z7hx47Rv377A9uyzz57WkACA9sf4TQhZWVnKyso64T5er1dJSUmuhwIAtH9heQ2osLBQCQkJuuCCCzRjxgwdOHCg2X3r6urk9/uDNgBA+xfyAho3bpyefvpprV27Vg888ICKioqUlZWlhoaGJvfPy8uTz+cLbKmpqaEeCQDQCoX894CuvfbawJ8HDRqkwYMHq0+fPiosLNSYMWOO2z83N1ezZs0KfO33+ykhADgDhP1t2L1791Z8fLxKS0ubvN/r9SomJiZoAwC0f2EvoD179ujAgQNKTk4O96EAAG2I8Y/gDh8+HHQ1U1ZWpq1btyouLk5xcXG65557NGHCBCUlJWnnzp268847df755yszMzOkgwMA2jbjAtq8ebMuu+yywNdfv34zdepULV68WNu2bdNTTz2lgwcPKiUlRWPHjtW9994rr9cbuqkBAG2ecQGNHj36hAtevvrqq6c1UHvl5veifvGLXxhnFi1aZJzx+XzGGbd2797dYscCbIiOjjbOTJs2zTizb98+44wkzZkzx1UuHFgLDgBgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFaE/CO50bQTrSDenISEBOPM448/bpzp06ePcUZyt+rvNz9+/VTV1tYaZ8477zzjjCT99a9/dZUDvjZ+/HjjzMiRI40zc+fONc5IUklJiatcOHAFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWsBhpC9m/f79x5u677zbOFBUVGWe6dOlinJGkgoIC48yf/vQn48yLL75onGlsbDTOSFJ2drZx5pVXXjHOJCYmGmeGDh1qnKmsrDTOSNKmTZuMM1999ZWrY7U3n376qXGmvr7eODNgwADjjCS9/fbbxhk3850KroAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoWI20hjuMYZ6KioowzpaWlxhm3nnjiCeOMm4VP//KXvxhnKioqjDOSdPnllxtn3Cx82qlTJ+PM5MmTjTOXXXaZcUaS3nzzTePMxIkTjTNuzl1kZKRx5ujRo8YZSfJ4PK5yptx8T++//76rY7XU93QquAICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACtYjLQV279/v3Fm7969LXIcSeratatxZs6cOcYZNwtWuvXSSy+1yHF2795tnFm2bJlxpl+/fsYZSXruueeMM1999ZVxxs2Cu7GxscaZyspK44wk9ejRwzhz1113GWeGDRtmnHnssceMM5J03333GWdeeOEFo/0dxzmlBZi5AgIAWEEBAQCsMCqgvLw8DRs2TNHR0UpISFB2drZKSkqC9qmtrVVOTo66du2qs88+WxMmTHB9+QsAaL+MCqioqEg5OTnasGGDXnvtNR09elRjx45VdXV1YJ9bb71VL730kpYtW6aioiLt3btXV199dcgHBwC0bUZvQlizZk3Q1wUFBUpISNCWLVs0atQoVVVV6W9/+5uWLl2qH/7wh5KkJ598UhdeeKE2bNig733ve6GbHADQpp3Wa0BVVVWSpLi4OEnSli1bdPToUWVkZAT26d+/v3r06KHi4uImH6Ourk5+vz9oAwC0f64LqLGxUbfccosuvfRSDRw4UJJUUVGhjh07Hvc2ycTERFVUVDT5OHl5efL5fIEtNTXV7UgAgDbEdQHl5ORo+/btrn5f4Jtyc3NVVVUV2MrLy0/r8QAAbYOrX0SdOXOmVq9erfXr16t79+6B25OSklRfX6+DBw8GXQVVVlYqKSmpycfyer3yer1uxgAAtGFGV0CO42jmzJlasWKF1q1bp169egXdP3ToUHXo0EFr164N3FZSUqLdu3crPT09NBMDANoFoyugnJwcLV26VKtWrVJ0dHTgdR2fz6dOnTrJ5/Pp+uuv16xZsxQXF6eYmBj96le/Unp6Ou+AAwAEMSqgxYsXS5JGjx4ddPuTTz6padOmSZIefvhhRUREaMKECaqrq1NmZqb+/Oc/h2RYAED7YVRAp7K4XFRUlPLz85Wfn+96KLjnpuxvu+22MEzSNI/H02LHas2OHDlinCksLDTOXHvttcYZSS326xBHjx41zrh5Dl1zzTXGGcnd+bv88suNM24WZb3wwguNM5J08cUXG2eWL19utL/jOKf0d8tacAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALDC1SeiwtzZZ59tnKmpqTHO1NbWGmfuu+8+44wkLVq0qEUy69atM8589dVXxpnWzs0nB3/zE4tN9O3b1zjz4YcfGmcyMzONM9nZ2caZ73//+8YZSTrnnHOMM25W625oaDDOvPnmm8YZSXrxxReNM6arlp/KJydIXAEBACyhgAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBWtdjHSs846y2hRvxEjRhgfY/PmzcYZSbrggguMM1OmTDHOrF692jgzceJE48zo0aONM5J04MAB48ynn35qnGmPC4u6WbDSzfOuW7duxhlJqqurc5UzdfvttxtnEhMTjTMff/yxcUaSduzY4SpnqqSkxDiTkpLi6linulDoN7l5vp7KcbgCAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArWu1ipBEREUYL4MXGxhof4+GHHzbOSFJaWppxJioqyjjzox/9yDhTW1trnFm2bJlxRpKGDRtmnKmvr3d1rPbGzYKQe/fuNc58+OGHxhlJeuihh4wz6enpxpl//etfxpmzzjL/Z6t79+7GGUlasGCBceazzz4zzhw+fNg4ExHh7vrBzb8Rbp6vp4IrIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwwuOEa5U5l/x+v3w+X4sc67rrrnOV+/jjj40zbhaF7Nu3r3HGDTffj+RuPjfHamhoMM60Ry+99JJxZuzYsa6O5Wahyx07dhhnZs+ebZyZNWuWcSYvL884I0mvvfaacaaV/ZNqVVVVlWJiYpq9nysgAIAVFBAAwAqjAsrLy9OwYcMUHR2thIQEZWdnq6SkJGif0aNHy+PxBG033nhjSIcGALR9RgVUVFSknJwcbdiwQa+99pqOHj2qsWPHqrq6Omi/6dOna9++fYHtwQcfDOnQAIC2z+ijBdesWRP0dUFBgRISErRlyxaNGjUqcHvnzp2VlJQUmgkBAO3Sab0GVFVVJUmKi4sLuv2ZZ55RfHy8Bg4cqNzcXNXU1DT7GHV1dfL7/UEbAKD9M/9w9f+vsbFRt9xyiy699FINHDgwcPvkyZPVs2dPpaSkaNu2bZo9e7ZKSkq0fPnyJh8nLy9P99xzj9sxAABtlOsCysnJ0fbt2/XWW28F3X7DDTcE/jxo0CAlJydrzJgx2rlzp/r06XPc4+Tm5ga9r9/v9ys1NdXtWACANsJVAc2cOVOrV6/W+vXr1b179xPum5aWJkkqLS1tsoC8Xq+8Xq+bMQAAbZhRATmOo1/96ldasWKFCgsL1atXr5Nmtm7dKklKTk52NSAAoH0yKqCcnBwtXbpUq1atUnR0tCoqKiRJPp9PnTp10s6dO7V06VJdccUV6tq1q7Zt26Zbb71Vo0aN0uDBg8PyDQAA2iajAlq8eLGkY79s+k1PPvmkpk2bpo4dO+r111/XwoULVV1drdTUVE2YMEFz5swJ2cAAgPbB+EdwJ5KamqqioqLTGggAcGY4o1fDBtqSyspK40yHDh1cHeuf//yncWbJkiXGmfHjxxtn6uvrjTP333+/cUbScau8wAyrYQMAWiUKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWOH6I7kBtKx//OMfxpl3333X1bFWrVplnKmrqzPOlJeXG2eOHDlinGFR0daJKyAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGBFq1sLznEc2yMArZKbNdDq6+tdHcvNf4duMo2NjS1yHNhxsr8rj9PK/jb37Nmj1NRU22MAAE5TeXm5unfv3uz9ra6AGhsbtXfvXkVHR8vj8QTd5/f7lZqaqvLycsXExFia0D7OwzGch2M4D8dwHo5pDefBcRwdOnRIKSkpioho/pWeVvcjuIiIiBM2piTFxMSc0U+wr3EejuE8HMN5OIbzcIzt8+Dz+U66D29CAABYQQEBAKxoUwXk9Xo1b948eb1e26NYxXk4hvNwDOfhGM7DMW3pPLS6NyEAAM4MbeoKCADQflBAAAArKCAAgBUUEADACgoIAGBFmymg/Px8nXfeeYqKilJaWpo2bdpke6QWN3/+fHk8nqCtf//+tscKu/Xr1+vKK69USkqKPB6PVq5cGXS/4ziaO3eukpOT1alTJ2VkZGjHjh12hg2jk52HadOmHff8GDdunJ1hwyQvL0/Dhg1TdHS0EhISlJ2drZKSkqB9amtrlZOTo65du+rss8/WhAkTVFlZaWni8DiV8zB69Ojjng833nijpYmb1iYK6Pnnn9esWbM0b948vffeexoyZIgyMzO1f/9+26O1uIsuukj79u0LbG+99ZbtkcKuurpaQ4YMUX5+fpP3P/jgg3rkkUe0ZMkSbdy4UV26dFFmZqZqa2tbeNLwOtl5kKRx48YFPT+effbZFpww/IqKipSTk6MNGzbotdde09GjRzV27FhVV1cH9rn11lv10ksvadmyZSoqKtLevXt19dVXW5w69E7lPEjS9OnTg54PDz74oKWJm+G0AcOHD3dycnICXzc0NDgpKSlOXl6exala3rx585whQ4bYHsMqSc6KFSsCXzc2NjpJSUnOH/7wh8BtBw8edLxer/Pss89amLBlfPs8OI7jTJ061bnqqquszGPL/v37HUlOUVGR4zjH/u47dOjgLFu2LLDPxx9/7EhyiouLbY0Zdt8+D47jOD/4wQ+cm2++2d5Qp6DVXwHV19dry5YtysjICNwWERGhjIwMFRcXW5zMjh07diglJUW9e/fWlClTtHv3btsjWVVWVqaKioqg54fP51NaWtoZ+fwoLCxUQkKCLrjgAs2YMUMHDhywPVJYVVVVSZLi4uIkSVu2bNHRo0eDng/9+/dXjx492vXz4dvn4WvPPPOM4uPjNXDgQOXm5qqmpsbGeM1qdathf9sXX3yhhoYGJSYmBt2emJioTz75xNJUdqSlpamgoEAXXHCB9u3bp3vuuUcjR47U9u3bFR0dbXs8KyoqKiSpyefH1/edKcaNG6err75avXr10s6dO3XXXXcpKytLxcXFioyMtD1eyDU2NuqWW27RpZdeqoEDB0o69nzo2LGjYmNjg/Ztz8+Hps6DJE2ePFk9e/ZUSkqKtm3bptmzZ6ukpETLly+3OG2wVl9A+D9ZWVmBPw8ePFhpaWnq2bOnXnjhBV1//fUWJ0NrcO211wb+PGjQIA0ePFh9+vRRYWGhxowZY3Gy8MjJydH27dvPiNdBT6S583DDDTcE/jxo0CAlJydrzJgx2rlzp/r06dPSYzap1f8ILj4+XpGRkce9i6WyslJJSUmWpmodYmNj1a9fP5WWltoexZqvnwM8P47Xu3dvxcfHt8vnx8yZM7V69Wq98cYbQZ8flpSUpPr6eh08eDBo//b6fGjuPDQlLS1NklrV86HVF1DHjh01dOhQrV27NnBbY2Oj1q5dq/T0dIuT2Xf48GHt3LlTycnJtkexplevXkpKSgp6fvj9fm3cuPGMf37s2bNHBw4caFfPD8dxNHPmTK1YsULr1q1Tr169gu4fOnSoOnToEPR8KCkp0e7du9vV8+Fk56EpW7dulaTW9Xyw/S6IU/Hcc885Xq/XKSgocD766CPnhhtucGJjY52Kigrbo7Wo2267zSksLHTKysqct99+28nIyHDi4+Od/fv32x4trA4dOuS8//77zvvvv+9IchYsWOC8//77zv/+9z/HcRzn/vvvd2JjY51Vq1Y527Ztc6666iqnV69ezpEjRyxPHlonOg+HDh1ybr/9dqe4uNgpKytzXn/9dee73/2u07dvX6e2ttb26CEzY8YMx+fzOYWFhc6+ffsCW01NTWCfG2+80enRo4ezbt06Z/PmzU56erqTnp5ucerQO9l5KC0tdX73u985mzdvdsrKypxVq1Y5vXv3dkaNGmV58mBtooAcx3EWLVrk9OjRw+nYsaMzfPhwZ8OGDbZHanGTJk1ykpOTnY4dOzrnnnuuM2nSJKe0tNT2WGH3xhtvOJKO26ZOneo4zrG3Yt99991OYmKi4/V6nTFjxjglJSV2hw6DE52HmpoaZ+zYsU63bt2cDh06OD179nSmT5/e7v4nranvX5Lz5JNPBvY5cuSIc9NNNznnnHOO07lzZ2f8+PHOvn377A0dBic7D7t373ZGjRrlxMXFOV6v1zn//POdO+64w6mqqrI7+LfweUAAACta/WtAAID2iQICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArPh/jNuypShUMbsAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 定义自定义数据集并准备数据"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T01:29:19.597045Z",
     "start_time": "2024-12-05T01:29:19.567900Z"
    }
   },
   "source": [
    "# 定义自定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = CustomDataset(train_data, train_labels)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 定义 CNN 模型"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T01:29:22.007352Z",
     "start_time": "2024-12-05T01:29:22.001354Z"
    }
   },
   "source": [
    "# 定义 CNN 模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(512 * 1 * 1, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)  # 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T01:29:23.792696Z",
     "start_time": "2024-12-05T01:29:23.785846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义扩展后的 CNN 模型\n",
    "class ExtendedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExtendedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(1024)\n",
    "        self.conv5 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(1024, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 10)  # 10 个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))  # 输出尺寸减半\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))  # 输出尺寸减半\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))  # 输出尺寸减半\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))             # 不进行池化，保持尺寸\n",
    "        x = torch.relu(self.bn5(self.conv5(x)))             # 不进行池化，保持尺寸\n",
    "        x = self.global_pool(x)                             # 全局平均池化到 1x1\n",
    "        x = x.view(x.size(0), -1)                           # 展平张量\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:51:42.440830Z",
     "start_time": "2024-12-04T23:51:42.434617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义 Vision Transformer 模型\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=28, patch_size=7, in_channels=1, num_classes=10, embed_dim=768, depth=12, num_heads=12, mlp_dim=3072):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = in_channels * patch_size * patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # 图像切块\n",
    "        self.patch_embeddings = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # 位置编码\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "\n",
    "        # 分类标识符\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # Transformer编码器层\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # 分类头\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # 初始化参数\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.patch_embeddings.weight)\n",
    "        nn.init.normal_(self.position_embeddings, std=0.02)\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, in_channels, img_size, img_size]\n",
    "        x = self.patch_embeddings(x)  # [batch_size, embed_dim, num_patches_sqrt, num_patches_sqrt]\n",
    "        x = x.flatten(2)  # [batch_size, embed_dim, num_patches]\n",
    "        x = x.transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "\n",
    "        # 扩展cls_token\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)  # [batch_size, 1, embed_dim]\n",
    "\n",
    "        # 拼接cls_token和图像块\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # [batch_size, num_patches + 1, embed_dim]\n",
    "\n",
    "        # 添加位置编码\n",
    "        x = x + self.position_embeddings  # [batch_size, num_patches + 1, embed_dim]\n",
    "\n",
    "        # 输入Transformer编码器\n",
    "        x = x.transpose(0, 1)  # [num_patches + 1, batch_size, embed_dim]\n",
    "        x = self.encoder(x)  # [num_patches + 1, batch_size, embed_dim]\n",
    "        x = x.transpose(0, 1)  # [batch_size, num_patches + 1, embed_dim]\n",
    "\n",
    "        # 取出cls_token对应的输出\n",
    "        x = x[:, 0, :]  # [batch_size, embed_dim]\n",
    "\n",
    "        # 分类\n",
    "        x = self.classifier(x)  # [batch_size, num_classes]\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 进行交叉验证训练和测试"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN交叉测试"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T23:37:33.920791Z",
     "start_time": "2024-12-04T23:28:05.290390Z"
    }
   },
   "source": [
    "# 交叉验证训练和测试\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=THE_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=THE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = ExtendedCNN().to(DEVICE)  # Move model to device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(15):  # Adjust the number of epochs as needed\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(DEVICE)  # Move data to device\n",
    "            labels = labels.to(DEVICE)  # Move labels to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(DEVICE)  # Move data to device\n",
    "                labels = labels.to(DEVICE)  # Move labels to device\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch + 1}, Validation Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Validation Accuracy: 88.02%\n",
      "Epoch 2, Validation Accuracy: 93.97%\n",
      "Epoch 3, Validation Accuracy: 94.50%\n",
      "Epoch 4, Validation Accuracy: 94.15%\n",
      "Epoch 5, Validation Accuracy: 95.46%\n",
      "Epoch 6, Validation Accuracy: 96.17%\n",
      "Epoch 7, Validation Accuracy: 95.95%\n",
      "Epoch 8, Validation Accuracy: 96.28%\n",
      "Epoch 9, Validation Accuracy: 96.28%\n",
      "Epoch 10, Validation Accuracy: 96.09%\n",
      "Epoch 11, Validation Accuracy: 96.80%\n",
      "Epoch 12, Validation Accuracy: 96.38%\n",
      "Epoch 13, Validation Accuracy: 96.77%\n",
      "Epoch 14, Validation Accuracy: 96.12%\n",
      "Epoch 15, Validation Accuracy: 96.79%\n",
      "Fold 2\n",
      "Epoch 1, Validation Accuracy: 87.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 18\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move data to device\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move labels to device\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T01:56:54.643336Z",
     "start_time": "2024-12-05T01:48:52.643084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "total_epochs_x = 15\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=THE_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=THE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = ExtendedCNN().to(DEVICE)  # Move model to device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(total_epochs_x):  # Adjust the number of epochs as needed\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(DEVICE)  # Move data to device\n",
    "            labels = labels.to(DEVICE)  # Move labels to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 在每个epoch结束后更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        # 可选：打印当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(DEVICE)  # Move data to device\n",
    "                labels = labels.to(DEVICE)  # Move labels to device\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch: 1\n",
      "Current Learning Rate: 0.000989\n",
      "Validation Accuracy: 84.09%\n",
      "Epoch: 2\n",
      "Current Learning Rate: 0.000957\n",
      "Validation Accuracy: 88.14%\n",
      "Epoch: 3\n",
      "Current Learning Rate: 0.000905\n",
      "Validation Accuracy: 91.38%\n",
      "Epoch: 4\n",
      "Current Learning Rate: 0.000835\n",
      "Validation Accuracy: 92.88%\n",
      "Epoch: 5\n",
      "Current Learning Rate: 0.000750\n",
      "Validation Accuracy: 91.95%\n",
      "Epoch: 6\n",
      "Current Learning Rate: 0.000655\n",
      "Validation Accuracy: 93.35%\n",
      "Epoch: 7\n",
      "Current Learning Rate: 0.000552\n",
      "Validation Accuracy: 92.16%\n",
      "Epoch: 8\n",
      "Current Learning Rate: 0.000448\n",
      "Validation Accuracy: 93.58%\n",
      "Epoch: 9\n",
      "Current Learning Rate: 0.000345\n",
      "Validation Accuracy: 93.36%\n",
      "Epoch: 10\n",
      "Current Learning Rate: 0.000250\n",
      "Validation Accuracy: 94.72%\n",
      "Epoch: 11\n",
      "Current Learning Rate: 0.000165\n",
      "Validation Accuracy: 95.11%\n",
      "Epoch: 12\n",
      "Current Learning Rate: 0.000095\n",
      "Validation Accuracy: 95.62%\n",
      "Epoch: 13\n",
      "Current Learning Rate: 0.000043\n",
      "Validation Accuracy: 95.91%\n",
      "Epoch: 14\n",
      "Current Learning Rate: 0.000011\n",
      "Validation Accuracy: 96.09%\n",
      "Epoch: 15\n",
      "Current Learning Rate: 0.000000\n",
      "Validation Accuracy: 96.29%\n",
      "Epoch: 16\n",
      "Current Learning Rate: 0.000011\n",
      "Validation Accuracy: 96.30%\n",
      "Epoch: 17\n",
      "Current Learning Rate: 0.000043\n",
      "Validation Accuracy: 96.25%\n",
      "Epoch: 18\n",
      "Current Learning Rate: 0.000095\n",
      "Validation Accuracy: 96.08%\n",
      "Epoch: 19\n",
      "Current Learning Rate: 0.000165\n",
      "Validation Accuracy: 95.83%\n",
      "Epoch: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 20\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move data to device\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move labels to device\u001B[39;00m\n\u001B[0;32m     22\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transformer交叉测试"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T01:19:23.052830Z",
     "start_time": "2024-12-05T00:14:22.166578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "total_epochs = 100\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=THE_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=THE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = VisionTransformer().to(DEVICE)  # Move model to device\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(total_epochs):  # Adjust the number of epochs as needed\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(DEVICE)  # Move data to device\n",
    "            labels = labels.to(DEVICE)  # Move labels to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 在每个epoch结束后更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        # 可选：打印当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(DEVICE)  # Move data to device\n",
    "                labels = labels.to(DEVICE)  # Move labels to device\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch + 1}, Validation Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.0009997532801828658\n",
      "Epoch 1, Validation Accuracy: 9.78%\n",
      "Current Learning Rate: 0.0009990133642141358\n",
      "Epoch 2, Validation Accuracy: 9.79%\n",
      "Current Learning Rate: 0.00099778098230154\n",
      "Epoch 3, Validation Accuracy: 9.80%\n",
      "Current Learning Rate: 0.000996057350657239\n",
      "Epoch 4, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009938441702975688\n",
      "Epoch 5, Validation Accuracy: 10.82%\n",
      "Current Learning Rate: 0.0009911436253643444\n",
      "Epoch 6, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009879583809693736\n",
      "Epoch 7, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009842915805643154\n",
      "Epoch 8, Validation Accuracy: 9.78%\n",
      "Current Learning Rate: 0.0009801468428384714\n",
      "Epoch 9, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009755282581475767\n",
      "Epoch 10, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009704403844771127\n",
      "Epoch 11, Validation Accuracy: 9.79%\n",
      "Current Learning Rate: 0.0009648882429441257\n",
      "Epoch 12, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009588773128419905\n",
      "Epoch 13, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009524135262330098\n",
      "Epoch 14, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009455032620941839\n",
      "Epoch 15, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009381533400219318\n",
      "Epoch 16, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009303710135019719\n",
      "Epoch 17, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009221639627510076\n",
      "Epoch 18, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.000913540287137281\n",
      "Epoch 19, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0009045084971874739\n",
      "Epoch 20, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0008950775061878453\n",
      "Epoch 21, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0008852566213878948\n",
      "Epoch 22, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0008750555348152299\n",
      "Epoch 23, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0008644843137107058\n",
      "Epoch 24, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0008535533905932738\n",
      "Epoch 25, Validation Accuracy: 11.02%\n",
      "Current Learning Rate: 0.0008422735529643445\n",
      "Epoch 26, Validation Accuracy: 11.02%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 19\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move data to device\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move labels to device\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 使用整个训练集训练模型并在测试集上进行推理"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN推理"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:10:32.464856Z",
     "start_time": "2024-12-05T02:10:09.624548Z"
    }
   },
   "source": [
    "# Train the model on the entire training set\n",
    "print(\"Training on the entire dataset\")\n",
    "train_loader_entire = DataLoader(train_dataset, batch_size=THE_BATCH_SIZE, shuffle=True)\n",
    "modelF = ExtendedCNN().to(DEVICE)  # Reinitialize the model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs)\n",
    "\n",
    "for epoch in range(15):  # Adjust the number of epochs as needed\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    modelF.train()\n",
    "    for images, labels in train_loader_entire:\n",
    "        images = images.to(DEVICE)  # Move data to device\n",
    "        labels = labels.to(DEVICE)  # Move labels to device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelF(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # 在每个epoch结束后更新学习率\n",
    "    #scheduler.step()\n",
    "\n",
    "    # 可选：打印当前学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "# Perform inference on the test dataset\n",
    "with open(TEST_PKL_FILE, 'rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "\n",
    "test_dataset = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=THE_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "modelF.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(DEVICE)  # Move test data to device\n",
    "        outputs = modelF(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Save the prediction results to a CSV file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': np.arange(1, len(predictions) + 1),\n",
    "    'class': predictions\n",
    "})\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "print(\"Prediction results saved to: \" + SUBMISSION_FILE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the entire dataset\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m modelF\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader_entire:\n\u001B[0;32m     13\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move data to device\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Move labels to device\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 212\u001B[0m         collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(batch, \u001B[38;5;241m0\u001B[39m, out\u001B[38;5;241m=\u001B[39mout)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
